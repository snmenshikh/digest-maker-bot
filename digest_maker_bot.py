import asyncio
import os
import io
import re
import logging
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Dict, Any, Optional

import requests
import pandas as pd
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

from docx import Document
from docx.shared import Pt

import nltk
from nltk.tokenize import sent_tokenize

from aiogram import Bot, Dispatcher, F
from aiogram.filters import CommandStart
from aiogram.types import (
    Message, CallbackQuery, InlineKeyboardMarkup, InlineKeyboardButton, FSInputFile
)
from aiogram.fsm.state import StatesGroup, State
from aiogram.fsm.context import FSMContext
from aiogram.fsm.storage.memory import MemoryStorage

# ---------- Logging ----------
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s - %(name)s: %(message)s"
)
logger = logging.getLogger("digest_maker_bot")

# ---------- NLTK bootstrap (safe) ----------
def ensure_nltk():
    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        nltk.download("punkt")

# ---------- Secure token retrieval ----------
def get_bot_token() -> str:
    """
    Securely fetch BOT_TOKEN from environment (Portainer -> Stack -> Environment).
    Never hardcode tokens in code or in the image.
    """
    token = os.getenv("BOT_TOKEN", "").strip()
    if not token:
        raise RuntimeError(
            "BOT_TOKEN is not set. Configure it via environment variables in Portainer."
        )
    return token

# ---------- FSM States ----------
class DigestStates(StatesGroup):
    WAITING_FOR_EXCEL = State()
    WAITING_FOR_INTERVAL = State()
    WAITING_FOR_KEYWORDS = State()
    PROCESSING = State()

# ---------- Excel validation ----------
TG_URL_RE = re.compile(r"^https://t\.me/([A-Za-z0-9_]+)/?$")

def read_channels_from_excel(file_bytes: bytes) -> List[Tuple[str, str, str]]:
    """
    Reads an Excel file (xls/xlsx). Expects:
      - Col A: Channel Name
      - Col B: Channel URL (https://t.me/<slug>)
    Returns list of tuples: (channel_name, channel_url, slug)
    Raises ValueError with human-friendly messages on problems.
    """
    try:
        # Try to read as Excel (pandas auto-detects xls/xlsx by content)
        df = pd.read_excel(io.BytesIO(file_bytes), header=None)
    except Exception as e:
        raise ValueError(
            f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å Excel. –ü—Ä–æ–≤–µ—Ä—å —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ (*.xls –∏–ª–∏ *.xlsx). –î–µ—Ç–∞–ª–∏: {e}"
        )

    if df.shape[1] < 2:
        raise ValueError("–í —Ç–∞–±–ª–∏—Ü–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–∏–Ω–∏–º—É–º 2 —Å—Ç–æ–ª–±—Ü–∞: A ‚Äî –∏–º—è –∫–∞–Ω–∞–ª–∞, B ‚Äî —Å—Å—ã–ª–∫–∞ https://t.me/<slug>.")

    channels: List[Tuple[str, str, str]] = []
    for idx, row in df.iterrows():
        name = str(row.iloc[0]).strip()
        url = str(row.iloc[1]).strip()
        if not name or not url:
            logger.warning(f"–°—Ç—Ä–æ–∫–∞ {idx+1}: –ø—Ä–æ–ø—É—â–µ–Ω–æ –∏–∑-–∑–∞ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.")
            continue

        m = TG_URL_RE.match(url)
        if not m:
            raise ValueError(
                f"–°—Ç—Ä–æ–∫–∞ {idx+1}: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ '{url}'. –û–∂–∏–¥–∞–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç https://t.me/<slug>"
            )
        slug = m.group(1)
        channels.append((name, url, slug))

    if not channels:
        raise ValueError("–í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ —Å –∫–∞–Ω–∞–ª–æ–º.")

    return channels

# ---------- Interval helpers ----------
def now_utc() -> datetime:
    return datetime.now(timezone.utc)

def interval_to_timedelta(interval_key: str) -> timedelta:
    """
    'day' -> 1 day, 'week' -> 7 days, 'month' -> 30 days
    """
    mapping = {
        "day": timedelta(days=1),
        "week": timedelta(days=7),
        "month": timedelta(days=30),
    }
    return mapping[interval_key]

def build_interval_keyboard() -> InlineKeyboardMarkup:
    return InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="–°—É—Ç–∫–∏", callback_data="interval:day")],
        [InlineKeyboardButton(text="–ù–µ–¥–µ–ª—è", callback_data="interval:week")],
        [InlineKeyboardButton(text="–ú–µ—Å—è—Ü", callback_data="interval:month")],
    ])

# ---------- Scraping ----------
HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/122.0.0.0 Safari/537.36"
}

def make_s_url(slug: str) -> str:
    return f"https://t.me/s/{slug}"

def parse_telegram_s_page(slug: str) -> List[Dict[str, Any]]:
    """
    Scrape https://t.me/s/<slug> and return a list of messages:
    Each message dict: { "text": str, "dt": datetime (UTC if possible) }
    Notes:
      - No JS ‚Üí we get only visible batch (recent posts).
      - Time is taken from <time datetime="..."> if present; otherwise None.
    """
    url = make_s_url(slug)
    r = requests.get(url, headers=HEADERS, timeout=20)
    if r.status_code != 200:
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url} (HTTP {r.status_code})")

    soup = BeautifulSoup(r.text, "lxml")

    messages = []
    # Telegram web structure usually wraps messages in 'tgme_widget_message' containers
    for msg in soup.select(".tgme_widget_message_wrap, .tgme_widget_message"):
        # Text
        text_el = msg.select_one(".tgme_widget_message_text")
        text = text_el.get_text(separator=" ", strip=True) if text_el else ""

        # Datetime
        dt_value: Optional[datetime] = None
        time_el = msg.find("time")
        if time_el and time_el.has_attr("datetime"):
            try:
                dt_value = dtparser.isoparse(time_el["datetime"])
                if dt_value.tzinfo is None:
                    dt_value = dt_value.replace(tzinfo=timezone.utc)
                else:
                    dt_value = dt_value.astimezone(timezone.utc)
            except Exception:
                dt_value = None

        if text:
            messages.append({"text": text, "dt": dt_value})

    return messages

# ---------- Keyword filtering & summarization ----------
def filter_messages_by_time_and_keywords(
    messages: List[Dict[str, Any]],
    since_dt: datetime,
    keywords: List[str]
) -> List[Dict[str, Any]]:
    """
    Keep messages newer than since_dt and matching keywords (case-insensitive).
    If keywords list is empty, keep all.
    """
    kws = [k.lower() for k in keywords if k.strip()]
    filtered = []
    for m in messages:
        dt_ok = (m["dt"] is None) or (m["dt"] >= since_dt)  # if no dt, we keep it cautiously
        if not dt_ok:
            continue
        if not kws:
            filtered.append(m)
            continue
        text_low = m["text"].lower()
        if any(kw in text_low for kw in kws):
            filtered.append(m)
    return filtered

def summarize_text_extractively(text: str, keywords: List[str], max_sentences: int = 3) -> str:
    """
    Simple extractive summary:
      1) Split into sentences (NLTK Punkt).
      2) Rank sentences: +2 if contains keyword, +1 per unique non-stopword token frequency (lightweight).
      3) Return top-N in original order.
    """
    if not text:
        return ""
    sentences = [s.strip() for s in sent_tokenize(text) if s.strip()]
    if len(sentences) <= max_sentences:
        return " ".join(sentences)

    kws = set(k.lower() for k in keywords if k.strip())

    # crude tokenization
    def tokens(s: str) -> List[str]:
        return re.findall(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë0-9_]+", s.lower())

    # frequency map
    freq: Dict[str, int] = {}
    for s in sentences:
        for t in set(tokens(s)):
            freq[t] = freq.get(t, 0) + 1

    # sentence scoring
    scored = []
    for i, s in enumerate(sentences):
        ts = tokens(s)
        score = 0
        if kws and any(k in s.lower() for k in kws):
            score += 2
        score += sum(freq.get(t, 0) for t in set(ts))
        scored.append((i, s, score))

    # pick top-K by score, keep original order
    top = sorted(scored, key=lambda x: x[2], reverse=True)[:max_sentences]
    top_sorted = sorted(top, key=lambda x: x[0])
    return " ".join(s for _, s, _ in top_sorted)

# ---------- DOCX generation ----------
def build_docx_digest(
    user_id: int,
    interval_label: str,
    keywords: List[str],
    results: Dict[str, Dict[str, Any]]
) -> str:
    """
    results: {
      channel_name: {
        "url": "...",
        "items": [ { "dt": datetime|None, "original": str, "summary": str } ]
      }, ...
    }
    Returns path to saved .docx
    """
    doc = Document()

    # Title
    title = doc.add_paragraph()
    run = title.add_run("–ö—Ä–∞—Ç–∫–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ Telegram-–∫–∞–Ω–∞–ª–∞–º")
    run.font.bold = True
    run.font.size = Pt(16)

    # Meta
    meta = doc.add_paragraph()
    meta.add_run(f"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω: ").bold = True
    meta.add_run(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    meta.add_run("\n–ò–Ω—Ç–µ—Ä–≤–∞–ª: ").bold = True
    meta.add_run(interval_label)
    meta.add_run("\n–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: ").bold = True
    meta.add_run(", ".join(keywords) if keywords else "‚Äî")

    # Body
    for ch_name, data in results.items():
        url = data["url"]
        items = data["items"]

        p = doc.add_paragraph()
        hdr = p.add_run(f"\n{ch_name}")
        hdr.font.bold = True
        hdr.font.size = Pt(13)

        doc.add_paragraph(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {ch_name} ({url})")

        if not items:
            doc.add_paragraph("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∑–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª.")
            continue

        for it in items:
            dt_str = it["dt"].astimezone().strftime("%Y-%m-%d %H:%M") if it["dt"] else "–¥–∞—Ç–∞ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞"
            doc.add_paragraph(f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {dt_str}")
            if it["summary"]:
                doc.add_paragraph(it["summary"])
            else:
                # fallback ‚Äî –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (—É—Å–µ—á—ë–Ω–Ω—ã–π)
                txt = it["original"]
                if len(txt) > 800:
                    txt = txt[:800] + "..."
                doc.add_paragraph(txt)
            doc.add_paragraph("---")

    fname = f"digest_{user_id}_{int(datetime.now().timestamp())}.docx"
    path = os.path.join(os.getcwd(), fname)
    doc.save(path)
    return path

# ---------- Bot setup ----------
async def on_start(message: Message, state: FSMContext):
    await state.clear()
    await message.answer(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –ø–æ–¥–≥–æ—Ç–æ–≤–ª—é –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ Telegram-–∫–∞–Ω–∞–ª–∞–º.\n"
        "–ü—Ä–∏—à–ª–∏—Ç–µ Excel-—Ñ–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –∫–∞–Ω–∞–ª–æ–≤:\n"
        "‚Ä¢ —Å—Ç–æ–ª–±–µ—Ü A ‚Äî –∏–º—è –∫–∞–Ω–∞–ª–∞\n"
        "‚Ä¢ —Å—Ç–æ–ª–±–µ—Ü B ‚Äî —Å—Å—ã–ª–∫–∞ –≤–∏–¥–∞ https://t.me/<slug>\n\n"
        "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è *.xls –∏ *.xlsx."
    )
    await state.set_state(DigestStates.WAITING_FOR_EXCEL)

async def on_excel(message: Message, state: FSMContext):
    if not message.document:
        await message.answer("–≠—Ç–æ –Ω–µ —Ñ–∞–π–ª. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–∏—à–ª–∏—Ç–µ Excel (*.xls –∏–ª–∏ *.xlsx).")
        return

    file_name = message.document.file_name or ""
    if not (file_name.endswith(".xls") or file_name.endswith(".xlsx")):
        await message.answer("–ù—É–∂–µ–Ω Excel-—Ñ–∞–π–ª (*.xls –∏–ª–∏ *.xlsx). –ü—Ä–∏—à–ª–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–∞–π–ª.")
        return

    try:
        # download file bytes
        file = await message.bot.get_file(message.document.file_id)
        file_bytes = await message.bot.download_file(file.file_path)
        content = file_bytes.read()

        # validate & parse
        channels = read_channels_from_excel(content)
        await state.update_data(channels=channels)

        await message.answer(
            "–§–∞–π–ª –ø—Ä–∏–Ω—è—Ç –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω ‚úÖ\n–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª:",
            reply_markup=build_interval_keyboard()
        )
        await state.set_state(DigestStates.WAITING_FOR_INTERVAL)

    except ValueError as ve:
        logger.exception("Excel validation failed")
        await message.answer(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Excel: {ve}")
    except Exception as e:
        logger.exception("Excel processing unexpected error")
        await message.answer(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏: {e}")

async def on_interval(callback: CallbackQuery, state: FSMContext):
    if not callback.data or not callback.data.startswith("interval:"):
        await callback.answer()
        return
    key = callback.data.split(":", 1)[1]  # 'day' | 'week' | 'month'
    if key not in ("day", "week", "month"):
        await callback.answer("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª")
        return

    await state.update_data(interval_key=key)
    await callback.message.answer(
        "–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä: –ò–ò, Python, –≤–∞–∫–∞–Ω—Å–∏–∏).\n"
        "–ú–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –ø—É—Å—Ç—ã–º ‚Äî —Ç–æ–≥–¥–∞ —è —Å–æ–±–µ—Ä—É –≤—Å—ë –ø–æ–¥—Ä—è–¥ –∏ –∫—Ä–∞—Ç–∫–æ —Ä–µ–∑—é–º–∏—Ä—É—é."
    )
    await state.set_state(DigestStates.WAITING_FOR_KEYWORDS)
    await callback.answer()

async def on_keywords(message: Message, state: FSMContext):
    await message.answer("–ü—Ä–∏–Ω—è–ª. –§–æ—Ä–º–∏—Ä—É—é –¥–∞–π–¥–∂–µ—Å—Ç, —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏‚Ä¶")
    await state.set_state(DigestStates.PROCESSING)

    data = await state.get_data()
    channels: List[Tuple[str, str, str]] = data.get("channels", [])
    interval_key: str = data.get("interval_key", "week")

    # parse keywords
    raw = (message.text or "").strip()
    keywords = [w.strip() for w in re.split(r"[,\n;]+", raw) if w.strip()]

    # Ensure NLTK
    ensure_nltk()

    try:
        # Time window
        since = now_utc() - interval_to_timedelta(interval_key)
        interval_label = {"day": "–°—É—Ç–∫–∏", "week": "–ù–µ–¥–µ–ª—è", "month": "–ú–µ—Å—è—Ü"}[interval_key]

        results: Dict[str, Dict[str, Any]] = {}

        for ch_name, ch_url, slug in channels:
            try:
                msgs = parse_telegram_s_page(slug)
                msgs = filter_messages_by_time_and_keywords(msgs, since, keywords)

                items = []
                for m in msgs:
                    summary = summarize_text_extractively(m["text"], keywords, max_sentences=3)
                    items.append({
                        "dt": m["dt"],
                        "original": m["text"],
                        "summary": summary
                    })

                results[ch_name] = {"url": ch_url, "items": items}

            except Exception as e:
                logger.exception(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–Ω–∞–ª–∞ {ch_name} ({ch_url})")
                results[ch_name] = {
                    "url": ch_url,
                    "items": [],
                }
                # –î–æ–±–∞–≤–∏–º "—Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ" –∫–∞–∫ —ç–ª–µ–º–µ–Ω—Ç, —á—Ç–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–∏–¥–µ–ª, —á—Ç–æ –∫–∞–Ω–∞–ª –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
                results[ch_name]["items"].append({
                    "dt": None,
                    "original": f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å {make_s_url(slug)}: {e}",
                    "summary": ""
                })

        # Build DOCX
        out_path = build_docx_digest(
            user_id=message.from_user.id,
            interval_label=interval_label,
            keywords=keywords,
            results=results
        )

        await message.answer_document(FSInputFile(out_path), caption="–ì–æ—Ç–æ–≤–æ. –í–∞—à –¥–∞–π–¥–∂–µ—Å—Ç üìÑ")
        await state.clear()

    except Exception as e:
        logger.exception("Unexpected processing error")
        await message.answer(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞: {e}")
        await state.clear()

# ---------- Entrypoint ----------
async def main():
    token = get_bot_token()
    bot = Bot(token=token)
    dp = Dispatcher(storage=MemoryStorage())

    dp.message.register(on_start, CommandStart())
    dp.message.register(on_excel, DigestStates.WAITING_FOR_EXCEL)
    dp.callback_query.register(on_interval, F.data.startswith("interval:"), DigestStates.WAITING_FOR_INTERVAL)
    dp.message.register(on_keywords, DigestStates.WAITING_FOR_KEYWORDS)

    logger.info("Bot started. Waiting for updates...")
    await dp.start_polling(bot, allowed_updates=["message", "callback_query"])

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except (KeyboardInterrupt, SystemExit):
        logger.info("Bot stopped.")
import os
import logging
import asyncio
from datetime import datetime, timedelta, timezone

import pandas as pd
import requests
from bs4 import BeautifulSoup

from aiogram import Bot, Dispatcher, types
from aiogram.types import Message, ReplyKeyboardMarkup, KeyboardButton
from aiogram.fsm.state import StatesGroup, State
from aiogram.fsm.context import FSMContext
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.filters import Command

import nltk
from nltk.tokenize import sent_tokenize

from docx import Document
from docx.shared import Pt
from docx.oxml.shared import OxmlElement, qn
import docx.opc.constants

# ---------- –õ–æ–≥–∏ ----------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("digest_maker_bot")

# ---------- FSM —Å–æ—Å—Ç–æ—è–Ω–∏—è ----------
class DigestStates(StatesGroup):
    WAITING_FOR_EXCEL = State()
    WAITING_FOR_INTERVAL = State()
    WAITING_FOR_KEYWORDS = State()

# ---------- –¢–æ–∫–µ–Ω ----------
BOT_TOKEN = os.getenv("BOT_TOKEN")
if not BOT_TOKEN:
    raise ValueError("BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –≤ Portainer.")

bot = Bot(token=BOT_TOKEN)
dp = Dispatcher(storage=MemoryStorage())

# ---------- NLTK ----------
nltk.download("punkt")

# ---------- DOCX: –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–∏ ----------
def add_hyperlink(paragraph, text, url):
    part = paragraph.part
    r_id = part.relate_to(
        url,
        docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK,
        is_external=True,
    )

    hyperlink = OxmlElement("w:hyperlink")
    hyperlink.set(qn("r:id"), r_id)

    new_run = OxmlElement("w:r")
    rPr = OxmlElement("w:rPr")

    rStyle = OxmlElement("w:rStyle")
    rStyle.set(qn("w:val"), "Hyperlink")
    rPr.append(rStyle)
    new_run.append(rPr)

    t = OxmlElement("w:t")
    t.text = text
    new_run.append(t)

    hyperlink.append(new_run)
    paragraph._p.append(hyperlink)
    return hyperlink

# ---------- –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ----------
def clean_text(text: str) -> str:
    return text.replace("\x00", "").replace("\u0000", "").strip()

# ---------- –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è ----------
def summarize_text_extractively(text, keywords, max_sentences=3):
    sentences = [s.strip() for s in sent_tokenize(text) if s.strip()]
    if not sentences:
        return ""
    if keywords:
        # –ø—Ä–∏–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        keywords = [k.lower() for k in keywords]
        scored = [s for s in sentences if any(k in s.lower() for k in keywords)]
    else:
        scored = sentences
    return " ".join(scored[:max_sentences])

# ---------- –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–Ω–∞–ª–∞ ----------
def parse_channel(url, keywords, start_dt):
    try:
        html = requests.get(url, timeout=15).text
        soup = BeautifulSoup(html, "html.parser")
        posts = soup.find_all("div", class_="tgme_widget_message_wrap")

        items = []
        seen = set()
        for post in posts:
            dt_node = post.find("time")
            dt = None
            if dt_node and dt_node.has_attr("datetime"):
                dt = datetime.fromisoformat(dt_node["datetime"].replace("Z", "+00:00"))
                if dt < start_dt:
                    continue

            text_node = post.find("div", class_="tgme_widget_message_text")
            if not text_node:
                continue
            text = clean_text(text_node.get_text(" "))

            post_id = post.get("data-post")
            post_url = None
            if post_id and "/" in post_id:
                channel_username, msg_id = post_id.split("/")
                post_url = f"https://t.me/{channel_username}/{msg_id}"

            summary = summarize_text_extractively(text, keywords, 3)
            if not summary:
                continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ—Å—Ç—ã –±–µ–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º

            key = (dt, summary)
            if key in seen:
                continue
            seen.add(key)

            items.append({
                "dt": dt,
                "summary": summary,
                "original": text,
                "post_url": post_url,
            })
        return items
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–Ω–∞–ª–∞ {url}: {e}")
        return []

# ---------- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è .docx ----------
def build_docx_digest(user_id, channels, keywords, interval_days):
    doc = Document()
    doc.add_heading("–î–∞–π–¥–∂–µ—Å—Ç –ø–æ Telegram-–∫–∞–Ω–∞–ª–∞–º", level=1)

    local_tz = timezone(timedelta(hours=5))
    doc.add_paragraph(f"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω: {datetime.now(local_tz).strftime('%Y-%m-%d %H:%M:%S')}")

    start_dt = datetime.now(timezone.utc) - timedelta(days=interval_days)

    for ch_name, url in channels:
        items = parse_channel(url, keywords, start_dt)
        if not items:
            continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–Ω–∞–ª—ã –±–µ–∑ –ø—É–±–ª–∏–∫–∞—Ü–∏–π

        hdr = doc.add_heading(ch_name, level=2)
        hdr.style.font.size = Pt(13)
        doc.add_paragraph(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {ch_name} ({url})")

        for it in items:
            dt_str = it["dt"].astimezone(local_tz).strftime("%Y-%m-%d %H:%M") if it["dt"] else "–¥–∞—Ç–∞ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞"
            doc.add_paragraph(f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {dt_str}")

            doc.add_paragraph(it["summary"])

            if it.get("post_url"):
                add_hyperlink(doc.add_paragraph(), "üîó –û—Ç–∫—Ä—ã—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª", it["post_url"])

            doc.add_paragraph("---")

    fname = f"digest_{user_id}_{int(datetime.now().timestamp())}.docx"
    path = os.path.join(os.getcwd(), fname)
    doc.save(path)
    return path

# ---------- –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ ----------
async def on_start(message: Message, state: FSMContext):
    await state.clear()
    await message.answer(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –ø–æ–¥–≥–æ—Ç–æ–≤–ª—é –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ Telegram-–∫–∞–Ω–∞–ª–∞–º.\n"
        "–ü—Ä–∏—à–ª–∏—Ç–µ Excel-—Ñ–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –∫–∞–Ω–∞–ª–æ–≤:\n"
        "‚Ä¢ —Å—Ç–æ–ª–±–µ—Ü A ‚Äî –∏–º—è –∫–∞–Ω–∞–ª–∞\n"
        "‚Ä¢ —Å—Ç–æ–ª–±–µ—Ü B ‚Äî —Å—Å—ã–ª–∫–∞ –≤–∏–¥–∞ https://t.me/<slug>\n\n"
        "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è *.xls –∏ *.xlsx."
    )
    await state.set_state(DigestStates.WAITING_FOR_EXCEL)

async def on_excel(message: Message, state: FSMContext):
    if not message.document:
        await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª Excel.")
        return

    file_path = f"temp_{message.from_user.id}.xlsx"
    await bot.download(message.document, destination=file_path)

    try:
        df = pd.read_excel(file_path, engine="openpyxl")
        if df.shape[1] < 2:
            raise ValueError("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 2 —Å—Ç–æ–ª–±—Ü–∞: –∏–º—è –∏ —Å—Å—ã–ª–∫—É.")
        channels = df.iloc[:, :2].dropna().values.tolist()
    except Exception as e:
        await message.answer(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è Excel: {e}")
        return
    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

    await state.update_data(channels=channels)

    kb = ReplyKeyboardMarkup(
        keyboard=[[KeyboardButton(text="–°—É—Ç–∫–∏"), KeyboardButton(text="–ù–µ–¥–µ–ª—è"), KeyboardButton(text="–ú–µ—Å—è—Ü")]],
        resize_keyboard=True,
        one_time_keyboard=True,
    )
    await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª –≤—Ä–µ–º–µ–Ω–∏:", reply_markup=kb)
    await state.set_state(DigestStates.WAITING_FOR_INTERVAL)

async def on_interval(message: Message, state: FSMContext):
    intervals = {"–°—É—Ç–∫–∏": 1, "–ù–µ–¥–µ–ª—è": 7, "–ú–µ—Å—è—Ü": 30}
    interval_days = intervals.get(message.text)
    if not interval_days:
        await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª –∏–∑ –∫–Ω–æ–ø–æ–∫.")
        return

    await state.update_data(interval_days=interval_days)
    await message.answer("–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):")
    await state.set_state(DigestStates.WAITING_FOR_KEYWORDS)

async def on_keywords(message: Message, state: FSMContext):
    data = await state.get_data()
    channels = data["channels"]
    interval_days = data["interval_days"]
    keywords = [k.strip().lower() for k in message.text.split(",") if k.strip()]

    try:
        out_path = build_docx_digest(message.from_user.id, channels, keywords, interval_days)
        await message.answer_document(types.FSInputFile(out_path))
        os.remove(out_path)
    except Exception as e:
        logger.error(f"Unexpected processing error: {e}", exc_info=True)
        await message.answer("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")

    await state.clear()

# ---------- –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ ----------
dp.message.register(on_start, Command(commands=["start"]))
dp.message.register(on_excel, DigestStates.WAITING_FOR_EXCEL)
dp.message.register(on_interval, DigestStates.WAITING_FOR_INTERVAL)
dp.message.register(on_keywords, DigestStates.WAITING_FOR_KEYWORDS)

# ---------- Main ----------
async def main():
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())